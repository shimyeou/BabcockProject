{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73be07ea-10c9-4fd9-94e5-3589d2e9d56f",
   "metadata": {},
   "source": [
    "## Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60379448-ec38-4032-a437-9bbe9cdb04c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting face_recognition\n",
      "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
      "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
      "     ---------------------------------------- 0.0/100.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.0/100.1 MB 8.5 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 2.6/100.1 MB 8.4 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 3.1/100.1 MB 8.0 MB/s eta 0:00:13\n",
      "     - -------------------------------------- 3.4/100.1 MB 4.4 MB/s eta 0:00:23\n",
      "     - -------------------------------------- 4.2/100.1 MB 4.9 MB/s eta 0:00:20\n",
      "     -- ------------------------------------- 5.8/100.1 MB 4.9 MB/s eta 0:00:20\n",
      "     -- ------------------------------------- 6.3/100.1 MB 5.1 MB/s eta 0:00:19\n",
      "     -- ------------------------------------- 7.3/100.1 MB 4.9 MB/s eta 0:00:20\n",
      "     --- ------------------------------------ 9.4/100.1 MB 5.2 MB/s eta 0:00:18\n",
      "     ---- ---------------------------------- 11.5/100.1 MB 5.7 MB/s eta 0:00:16\n",
      "     ----- --------------------------------- 12.8/100.1 MB 5.8 MB/s eta 0:00:15\n",
      "     ----- --------------------------------- 14.7/100.1 MB 6.1 MB/s eta 0:00:15\n",
      "     ------ -------------------------------- 16.0/100.1 MB 6.1 MB/s eta 0:00:14\n",
      "     ------ -------------------------------- 17.8/100.1 MB 6.2 MB/s eta 0:00:14\n",
      "     ------- ------------------------------- 18.4/100.1 MB 6.0 MB/s eta 0:00:14\n",
      "     ------- ------------------------------- 20.2/100.1 MB 6.2 MB/s eta 0:00:13\n",
      "     -------- ------------------------------ 22.0/100.1 MB 6.4 MB/s eta 0:00:13\n",
      "     --------- ----------------------------- 24.1/100.1 MB 6.6 MB/s eta 0:00:12\n",
      "     ---------- ---------------------------- 26.2/100.1 MB 6.7 MB/s eta 0:00:12\n",
      "     ---------- ---------------------------- 28.0/100.1 MB 6.8 MB/s eta 0:00:11\n",
      "     ----------- --------------------------- 29.6/100.1 MB 6.9 MB/s eta 0:00:11\n",
      "     ------------ -------------------------- 30.9/100.1 MB 6.8 MB/s eta 0:00:11\n",
      "     ------------ -------------------------- 32.2/100.1 MB 6.8 MB/s eta 0:00:11\n",
      "     ------------- ------------------------- 34.3/100.1 MB 6.9 MB/s eta 0:00:10\n",
      "     -------------- ------------------------ 36.7/100.1 MB 7.0 MB/s eta 0:00:10\n",
      "     --------------- ----------------------- 38.5/100.1 MB 7.1 MB/s eta 0:00:09\n",
      "     --------------- ----------------------- 40.1/100.1 MB 7.2 MB/s eta 0:00:09\n",
      "     ---------------- ---------------------- 42.5/100.1 MB 7.3 MB/s eta 0:00:08\n",
      "     ----------------- --------------------- 44.0/100.1 MB 7.3 MB/s eta 0:00:08\n",
      "     ----------------- --------------------- 45.9/100.1 MB 7.4 MB/s eta 0:00:08\n",
      "     ------------------ -------------------- 47.7/100.1 MB 7.4 MB/s eta 0:00:08\n",
      "     ------------------- ------------------- 49.8/100.1 MB 7.5 MB/s eta 0:00:07\n",
      "     ------------------- ------------------- 50.9/100.1 MB 7.4 MB/s eta 0:00:07\n",
      "     -------------------- ------------------ 52.2/100.1 MB 7.3 MB/s eta 0:00:07\n",
      "     --------------------- ----------------- 54.5/100.1 MB 7.5 MB/s eta 0:00:07\n",
      "     --------------------- ----------------- 55.3/100.1 MB 7.3 MB/s eta 0:00:07\n",
      "     ---------------------- ---------------- 57.7/100.1 MB 7.4 MB/s eta 0:00:06\n",
      "     ---------------------- ---------------- 59.0/100.1 MB 7.4 MB/s eta 0:00:06\n",
      "     ----------------------- --------------- 61.1/100.1 MB 7.5 MB/s eta 0:00:06\n",
      "     ------------------------ -------------- 62.9/100.1 MB 7.5 MB/s eta 0:00:05\n",
      "     ------------------------- ------------- 64.5/100.1 MB 7.5 MB/s eta 0:00:05\n",
      "     -------------------------- ------------ 66.8/100.1 MB 7.6 MB/s eta 0:00:05\n",
      "     -------------------------- ------------ 68.4/100.1 MB 7.6 MB/s eta 0:00:05\n",
      "     -------------------------- ------------ 69.2/100.1 MB 7.6 MB/s eta 0:00:05\n",
      "     --------------------------- ----------- 71.0/100.1 MB 7.5 MB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 73.4/100.1 MB 7.6 MB/s eta 0:00:04\n",
      "     ----------------------------- --------- 75.2/100.1 MB 7.6 MB/s eta 0:00:04\n",
      "     ----------------------------- --------- 76.5/100.1 MB 7.6 MB/s eta 0:00:04\n",
      "     ------------------------------ -------- 78.9/100.1 MB 7.6 MB/s eta 0:00:03\n",
      "     ------------------------------- ------- 80.7/100.1 MB 7.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ------ 83.1/100.1 MB 7.7 MB/s eta 0:00:03\n",
      "     --------------------------------- ----- 85.2/100.1 MB 7.8 MB/s eta 0:00:02\n",
      "     --------------------------------- ----- 87.0/100.1 MB 7.8 MB/s eta 0:00:02\n",
      "     --------------------------------- ----- 87.0/100.1 MB 7.8 MB/s eta 0:00:02\n",
      "     --------------------------------- ----- 87.0/100.1 MB 7.8 MB/s eta 0:00:02\n",
      "     --------------------------------- ----- 87.0/100.1 MB 7.8 MB/s eta 0:00:02\n",
      "     --------------------------------- ----- 87.0/100.1 MB 7.8 MB/s eta 0:00:02\n",
      "     --------------------------------- ----- 87.3/100.1 MB 7.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- ---- 89.1/100.1 MB 7.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- --- 90.4/100.1 MB 7.1 MB/s eta 0:00:02\n",
      "     ----------------------------------- --- 92.3/100.1 MB 7.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ -- 94.1/100.1 MB 7.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 96.2/100.1 MB 7.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 97.5/100.1 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  98.6/100.1 MB 7.2 MB/s eta 0:00:01\n",
      "     -------------------------------------  100.1/100.1 MB 7.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 100.1/100.1 MB 7.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from face_recognition) (8.1.7)\n",
      "Collecting dlib>=19.7 (from face_recognition)\n",
      "  Downloading dlib-20.0.0.tar.gz (3.3 MB)\n",
      "     ---------------------------------------- 0.0/3.3 MB ? eta -:--:--\n",
      "     ---------------------- ----------------- 1.8/3.3 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.3/3.3 MB 8.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\esco\\anaconda3\\lib\\site-packages (from face_recognition) (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\esco\\anaconda3\\lib\\site-packages (from face_recognition) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\esco\\anaconda3\\lib\\site-packages (from Click>=6.0->face_recognition) (0.4.6)\n",
      "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: dlib, face-recognition-models\n",
      "  Building wheel for dlib (pyproject.toml): started\n",
      "  Building wheel for dlib (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for face-recognition-models (setup.py): started\n",
      "  Building wheel for face-recognition-models (setup.py): finished with status 'done'\n",
      "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566178 sha256=435d7507d7f883b6a93f8b04412b85783a1b1a5d532bed24bd71ca395c454e02\n",
      "  Stored in directory: c:\\users\\esco\\appdata\\local\\pip\\cache\\wheels\\8f\\47\\c8\\f44c5aebb7507f7c8a2c0bd23151d732d0f0bd6884ad4ac635\n",
      "Successfully built face-recognition-models\n",
      "Failed to build dlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for dlib (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [41 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  \n",
      "  ================================================================================\n",
      "  ================================================================================\n",
      "  ================================================================================\n",
      "  \n",
      "                     CMake is not installed on your system!\n",
      "  \n",
      "      Or it is possible some broken copy of cmake is installed on your system.\n",
      "      It is unfortunately very common for python package managers to include\n",
      "      broken copies of cmake.  So if the error above this refers to some file\n",
      "      path to a cmake file inside a python or anaconda or miniconda path then you\n",
      "      should delete that broken copy of cmake from your computer.\n",
      "  \n",
      "      Instead, please get an official copy of cmake from one of these known good\n",
      "      sources of an official cmake:\n",
      "          - cmake.org (this is how windows users should get cmake)\n",
      "          - apt install cmake (for Ubuntu or Debian based systems)\n",
      "          - yum install cmake (for Redhat or CenOS based systems)\n",
      "  \n",
      "      On a linux machine you can run `which cmake` to see what cmake you are\n",
      "      actually using.  If it tells you it's some cmake from any kind of python\n",
      "      packager delete it and install an official cmake.\n",
      "  \n",
      "      More generally, cmake is not installed if when you open a terminal window\n",
      "      and type\n",
      "         cmake --version\n",
      "      you get an error.  So you can use that as a very basic test to see if you\n",
      "      have cmake installed.  That is, if cmake --version doesn't run from the\n",
      "      same terminal window from which you are reading this error message, then\n",
      "      you have not installed cmake.  Windows users should take note that they\n",
      "      need to tell the cmake installer to add cmake to their PATH.  Since you\n",
      "      can't run commands that are not in your PATH.  This is how the PATH works\n",
      "      on Linux as well, but failing to add cmake to the PATH is a particularly\n",
      "      common problem on windows and rarely a problem on Linux.\n",
      "  \n",
      "  ================================================================================\n",
      "  ================================================================================\n",
      "  ================================================================================\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for dlib\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (dlib)\n"
     ]
    }
   ],
   "source": [
    "!pip install face_recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bf31ae-71e6-44a4-8e9f-61585166d3fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.21-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting jax (from mediapipe)\n",
      "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Downloading jaxlib-0.6.1-cp312-cp312-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\esco\\anaconda3\\lib\\site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.5.2-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting sentencepiece (from mediapipe)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Collecting ml_dtypes>=0.5.0 (from jax->mediapipe)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Collecting opt_einsum (from jax->mediapipe)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\esco\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\esco\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Downloading mediapipe-0.10.21-cp312-cp312-win_amd64.whl (51.0 MB)\n",
      "   ---------------------------------------- 0.0/51.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/51.0 MB 8.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 3.7/51.0 MB 9.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 5.5/51.0 MB 9.3 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 7.3/51.0 MB 9.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 9.4/51.0 MB 9.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 11.3/51.0 MB 9.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 12.6/51.0 MB 8.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 14.2/51.0 MB 8.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 16.5/51.0 MB 9.1 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 18.6/51.0 MB 9.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 20.7/51.0 MB 9.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 23.1/51.0 MB 9.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 24.9/51.0 MB 9.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 26.5/51.0 MB 9.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 28.3/51.0 MB 9.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 30.4/51.0 MB 9.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 32.5/51.0 MB 9.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 34.6/51.0 MB 9.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 37.0/51.0 MB 9.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 39.1/51.0 MB 9.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 41.2/51.0 MB 9.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 43.5/51.0 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 46.1/51.0 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 48.5/51.0 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  50.6/51.0 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 51.0/51.0 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading sounddevice-0.5.2-py3-none-win_amd64.whl (363 kB)\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading jax-0.6.1-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 9.1 MB/s eta 0:00:00\n",
      "Downloading jaxlib-0.6.1-cp312-cp312-win_amd64.whl (56.9 MB)\n",
      "   ---------------------------------------- 0.0/56.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.3/56.9 MB 7.4 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 3.7/56.9 MB 8.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 6.0/56.9 MB 9.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 8.4/56.9 MB 10.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 10.2/56.9 MB 9.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 12.3/56.9 MB 10.0 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 14.4/56.9 MB 10.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 16.8/56.9 MB 10.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 19.1/56.9 MB 10.1 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 21.2/56.9 MB 10.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 23.1/56.9 MB 10.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 24.4/56.9 MB 9.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 26.7/56.9 MB 9.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 28.3/56.9 MB 9.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 29.6/56.9 MB 9.5 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 32.0/56.9 MB 9.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 34.1/56.9 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 35.9/56.9 MB 9.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 37.7/56.9 MB 9.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 39.8/56.9 MB 9.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 41.7/56.9 MB 9.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 44.0/56.9 MB 9.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 45.9/56.9 MB 9.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 47.7/56.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 49.8/56.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 51.9/56.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 54.0/56.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  55.8/56.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.9/56.9 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.1/46.2 MB 10.7 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 4.2/46.2 MB 10.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 6.6/46.2 MB 10.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 8.4/46.2 MB 10.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 10.7/46.2 MB 10.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 12.3/46.2 MB 10.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 13.9/46.2 MB 10.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 15.7/46.2 MB 9.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 17.8/46.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 19.7/46.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 21.5/46.2 MB 9.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 23.3/46.2 MB 9.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 25.7/46.2 MB 9.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 27.5/46.2 MB 9.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 29.6/46.2 MB 9.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 31.7/46.2 MB 9.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 34.1/46.2 MB 9.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 36.2/46.2 MB 9.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 38.0/46.2 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.6/46.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 41.4/46.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.3/46.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.9/46.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 992.0/992.0 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: sentencepiece, flatbuffers, opt_einsum, opencv-contrib-python, ml_dtypes, absl-py, sounddevice, jaxlib, jax, mediapipe\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Esco\\\\anaconda3\\\\Lib\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c9732e-cee2-4a0c-8a4e-984f22f2032d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py): started\n",
      "  Building wheel for imutils (setup.py): finished with status 'done'\n",
      "  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25857 sha256=a8bee7d336691da755653fa5e4e42265e6a1bebed49a71b7c5820aec388ae58e\n",
      "  Stored in directory: c:\\users\\esco\\appdata\\local\\pip\\cache\\wheels\\5b\\76\\96\\ad0c321506837bef578cf3008df3916c23018435a355d9f6b1\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eaa1d0e-0e11-42d5-8f45-d2a90ca46c5d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\yeou1\\anaconda3\\envs\\mediapipe-env\\lib\\site-packages (4.11.0.86)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\yeou1\\anaconda3\\envs\\mediapipe-env\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6b370c-5f88-48d0-8e2b-9060b0350e2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0df2d1-c567-45bb-b33d-7ce803cdb31a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yeou1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e542d78-2dc2-40fa-9e4b-0a37dd758dd0",
   "metadata": {},
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31a085-83d3-4518-ae7d-bcf392c58a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "from threading import Thread\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    # compute the euclidean distances between the two sets of\n",
    "    # vertical eye landmarks (x, y)-coordinates\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "    # compute the euclidean distance between the horizontal\n",
    "    # eye landmark (x, y)-coordinates\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "    # compute the eye aspect ratio\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "\n",
    "    # return the eye aspect ratio\n",
    "    return ear\n",
    "\n",
    "# manually set the webcam index\n",
    "args = {\"webcam\": 0}  \n",
    "\n",
    "# define two constants, one for the eye aspect ratio to indicate\n",
    "# blink and another constant for the number of consecutive\n",
    "# frames the eye must be below the threshold for to set off the alarm\n",
    "EYE_AR_THRESH = 0.2\n",
    "EYE_AR_CONSEC_FRAMES = 15\n",
    "\n",
    "# initialize the frame counter as well as a boolean used to\n",
    "# indicate if the alarm is on\n",
    "COUNTER = 0\n",
    "ALARM_ON = False\n",
    "\n",
    "# start the video stream thread\n",
    "print(\"[INFO] starting video stream thread...\")\n",
    "vs = VideoStream(src=args[\"webcam\"]).start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "# loop over frames from the video stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video file stream, resize it, and convert it to grayscale\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=450)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # detect face landmarks\n",
    "    face_landmarks_list = face_recognition.face_landmarks(rgb_frame)\n",
    "\n",
    "    # loop over the face detections\n",
    "    for face_landmarks in face_landmarks_list:\n",
    "        # extract the left and right eye coordinates\n",
    "        leftEye = face_landmarks['left_eye']\n",
    "        rightEye = face_landmarks['right_eye']\n",
    "\n",
    "        # convert eye coordinates to NumPy arrays\n",
    "        leftEye = np.array(leftEye)\n",
    "        rightEye = np.array(rightEye)\n",
    "\n",
    "        # compute the eye aspect ratio for both eyes\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "        # average the eye aspect ratio together for both eyes\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "        # compute the convex hull for the left and right eye, then visualize each of the eyes\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "        # check to see if the eye aspect ratio is below the blink threshold\n",
    "        if ear < EYE_AR_THRESH:\n",
    "            COUNTER += 1\n",
    "\n",
    "            # if the eyes were closed for a sufficient number of frames, trigger the alarm\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                # if the alarm is not on, turn it on\n",
    "                if not ALARM_ON:\n",
    "                    ALARM_ON = True\n",
    "\n",
    "                # draw an alarm message on the frame\n",
    "                cv2.putText(frame, \"DROWSINESS ALERT!\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # otherwise, the eye aspect ratio is not below the threshold, reset the counter and alarm\n",
    "        else:\n",
    "            COUNTER = 0\n",
    "            ALARM_ON = False\n",
    "\n",
    "        # draw the computed eye aspect ratio on the frame for debugging\n",
    "        cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002865d2-b88b-4534-ba65-ad8dcb73d325",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Edited Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0c3a06-dde2-49db-8181-42cbc4c4d76a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance \u001b[38;5;28;01mas\u001b[39;00m dist\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VideoStream\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mface_recognition\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "from threading import Thread\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    \n",
    "    return ear\n",
    "\n",
    "def mouth_aspect_ratio(mouth, face_bbox):\n",
    "    A = dist.euclidean(mouth[2], mouth[10])  # Outer top lip (51) to outer bottom lip (59)\n",
    "    face_height = face_bbox[2] - face_bbox[0]  # bottom - top\n",
    "    mar = A / face_height  # Normalize MAR by face height\n",
    "\n",
    "    return mar\n",
    "\n",
    "# Manually set the webcam index\n",
    "args = {\"webcam\": 0}\n",
    "\n",
    "EYE_AR_THRESH = 0.2\n",
    "EYE_AR_CONSEC_FRAMES = 10\n",
    "MAR_THRESH = 0.4\n",
    "MOUTH_AR_CONSEC_FRAMES = 5\n",
    "FRAME_SKIP = 2  # Skip every 2nd frame\n",
    "frame_count = 0\n",
    "\n",
    "COUNTER = 0\n",
    "ALARM_ON = False\n",
    "face_landmarks_list = []\n",
    "\n",
    "print(\"[INFO] starting video stream thread...\")\n",
    "vs = VideoStream(src=args[\"webcam\"]).start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "def detect_face_landmarks(rgb_frame):\n",
    "    global face_landmarks_list\n",
    "    face_landmarks_list = face_recognition.face_landmarks(rgb_frame)\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    \n",
    "    # Increment frame count for skipping frames logic\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames based on FRAME_SKIP setting\n",
    "    if frame_count % FRAME_SKIP != 0:\n",
    "        continue  # Skip this frame and move to the next\n",
    "\n",
    "    # After skipping, continue with processing the current frame\n",
    "    frame = imutils.resize(frame, width=720)  # Reduce resolution\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Start face detection in a separate thread for the current frame\n",
    "    face_thread = Thread(target=detect_face_landmarks, args=(rgb_frame,))\n",
    "    face_thread.start()\n",
    "    \n",
    "    for (top, right, bottom, left), face_landmarks in zip(face_locations, face_landmarks_list):\n",
    "        # Extract the face bounding box\n",
    "        face_bbox = (left, top, right, bottom)\n",
    "\n",
    "        # Extract the landmarks\n",
    "        leftEye = np.array(face_landmarks['left_eye'])\n",
    "        rightEye = np.array(face_landmarks['right_eye'])\n",
    "        mouth = np.array(face_landmarks['top_lip'] + face_landmarks['bottom_lip'])\n",
    "\n",
    "        # Compute EAR (Eye Aspect Ratio\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "        # Compute MAR (Mouth Aspect Ratio) with face bounding box\n",
    "        mar = mouth_aspect_ratio(mouth, face_bbox)\n",
    "\n",
    "        #leftEyeHull = cv2.convexHull(leftEye)\n",
    "        #rightEyeHull = cv2.convexHull(rightEye)\n",
    "        #cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        #cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "        warning_text_1 = \"\"\n",
    "        warning_text_2 = \"\"\n",
    "        \n",
    "        if ear < EYE_AR_THRESH:\n",
    "            COUNTER += 1\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                ALARM_ON = True\n",
    "                warning_text_1 = \"DROWSINESS ALERT!\"\n",
    "        \n",
    "        else:\n",
    "            COUNTER = 0\n",
    "            ALARM_ON = False\n",
    "        \n",
    "        if mar > MAR_THRESH:\n",
    "            COUNTER += 1\n",
    "            if COUNTER >= MOUTH_AR_CONSEC_FRAMES:\n",
    "                ALARM_ON = True\n",
    "            warning_text_2 = \"YAWNING DETECTED!\"\n",
    "        \n",
    "        if warning_text_1:\n",
    "            cv2.putText(frame, warning_text_1, (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        if warning_text_2:\n",
    "            cv2.putText(frame, warning_text_2, (10, 60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        # Re-add EAR and MAR display\n",
    "        cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"MAR: {:.2f}\".format(mar), (300, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    # if the q key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "vs.stream.release()\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed25e4-2832-4736-bab3-8c343cb2e329",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mediapipe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d424ea1-8e4e-466d-8170-b1e995f94e3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance \u001b[38;5;28;01mas\u001b[39;00m dist\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Constants\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 15\n",
    "MAR_THRESH = 0.09# Adjust based on testing\n",
    "MAR_COUNTER = 0  # Add a separate counter for yawning\n",
    "MAR_CONSEC_FRAMES = 15\n",
    "FRAME_SKIP = 2  # Skip every 2nd frame to improve performance\n",
    "frame_count = 0\n",
    "COUNTER = 0\n",
    "ALARM_ON = False\n",
    "\n",
    "\n",
    "# Counters for warnings\n",
    "drowsiness_count = 0\n",
    "yawning_count = 0\n",
    "\n",
    "# Store timestamps of warnings\n",
    "drowsiness_timestamps = []\n",
    "yawning_timestamps = []\n",
    "\n",
    "# EAR Calculation\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# MAR Calculation (using outer lip landmarks only)\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = dist.euclidean(mouth[0], mouth[1])  # Outer top lip (point 13) to bottom lip (point 14)\n",
    "    return A  # No need to normalize with face height in this case\n",
    "\n",
    "# Start video stream\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames based on FRAME_SKIP setting\n",
    "    if frame_count % FRAME_SKIP != 0:\n",
    "        continue  # Skip this frame and move to the next\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # Flip for a mirror effect\n",
    "    frame = cv2.resize(frame, (720, 480))\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect facial landmarks\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Extract Eye and Mouth landmarks\n",
    "            landmarks = face_landmarks.landmark\n",
    "\n",
    "            # Eye indices based on MediaPipe Face Mesh\n",
    "            left_eye_idx = [33, 160, 158, 133, 153, 144]  # Example points\n",
    "            right_eye_idx = [263, 387, 385, 362, 380, 373]\n",
    "\n",
    "            leftEye = np.array([(landmarks[i].x, landmarks[i].y) for i in left_eye_idx])\n",
    "            rightEye = np.array([(landmarks[i].x, landmarks[i].y) for i in right_eye_idx])\n",
    "\n",
    "            # Mouth indices (using outer lips)\n",
    "            mouth_idx = [13, 14]  # Outer top and bottom lips\n",
    "            mouth = np.array([(landmarks[i].x, landmarks[i].y) for i in mouth_idx])\n",
    "\n",
    "            # Compute EAR & MAR\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "            mar = mouth_aspect_ratio(mouth)\n",
    "\n",
    "            # Debugging: Print EAR and MAR\n",
    "            #print(f\"EAR: {ear:.2f}, MAR: {mar:.2f}\")\n",
    "\n",
    "            # Display Warnings\n",
    "            warning_text_1 = \"\"\n",
    "            warning_text_2 = \"\"\n",
    "\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "                if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                    ALARM_ON = True\n",
    "                    warning_text_1 = \"DROWSINESS ALERT!\"\n",
    "\n",
    "                # Log the occurrence\n",
    "                drowsiness_count += 1\n",
    "                drowsiness_timestamps.append(time.strftime(\"%H:%M:%S\"))\n",
    "                \n",
    "            else:\n",
    "                COUNTER = 0\n",
    "                ALARM_ON = False\n",
    "\n",
    "            if mar > MAR_THRESH:\n",
    "                MAR_COUNTER += 1\n",
    "                if MAR_COUNTER >= MAR_CONSEC_FRAMES:\n",
    "                    warning_text_2 = \"YAWNING DETECTED!\"\n",
    "                    \n",
    "                # Log the occurrence\n",
    "                yawning_count += 1\n",
    "                yawning_timestamps.append(time.strftime(\"%H:%M:%S\"))\n",
    "                \n",
    "            else:\n",
    "                MAR_COUNTER = 0\n",
    "\n",
    "            if warning_text_1:\n",
    "                cv2.putText(frame, warning_text_1, (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            if warning_text_2:\n",
    "                cv2.putText(frame, warning_text_2, (10, 60),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            # Display EAR & MAR values\n",
    "            cv2.putText(frame, f\"EAR: {ear:.2f}\", (500, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"MAR: {mar:.2f}\", (500, 60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Fatigue Detection\", frame)\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "print(\"\\n========== FATIGUE REPORT ==========\")\n",
    "print(f\"Total Drowsiness Alerts: {drowsiness_count}\")\n",
    "print(f\"Total Yawning Alerts: {yawning_count}\")\n",
    "\n",
    "if drowsiness_count > 0:\n",
    "    print(\"\\nDrowsiness detected at:\")\n",
    "    for timestamp in drowsiness_timestamps:\n",
    "        print(f\" - {timestamp}\")\n",
    "\n",
    "if yawning_count > 0:\n",
    "    print(\"\\nYawning detected at:\")\n",
    "    for timestamp in yawning_timestamps:\n",
    "        print(f\" - {timestamp}\")\n",
    "\n",
    "print(\"\\n====================================\")\n",
    "\n",
    "# Cleanup\n",
    "vs.stream.release()\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54310026-eb78-4ad1-b44c-8fb6bb235a5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mediapipe report fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2142fb-068f-44a8-ac16-499c43594412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== FATIGUE REPORT ==========\n",
      "Total Drowsiness Alerts: 0\n",
      "Total Yawning Alerts: 0\n",
      "====================================\n",
      "Report saved as fatigue_report.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Constants\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 15\n",
    "MAR_THRESH = 0.09\n",
    "MAR_CONSEC_FRAMES = 15\n",
    "FRAME_SKIP = 2\n",
    "\n",
    "# Counters and flags\n",
    "frame_count = 0\n",
    "COUNTER = 0\n",
    "MAR_COUNTER = 0\n",
    "ALARM_ON = False\n",
    "yawn_alarm_on = False\n",
    "\n",
    "# Log counters and timestamps\n",
    "drowsiness_count = 0\n",
    "yawning_count = 0\n",
    "drowsiness_timestamps = []\n",
    "yawning_timestamps = []\n",
    "\n",
    "# EAR Calculation\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# MAR Calculation\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = dist.euclidean(mouth[0], mouth[1])\n",
    "    return A\n",
    "\n",
    "# Start video stream\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    frame_count += 1\n",
    "\n",
    "    if frame_count % FRAME_SKIP != 0:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame = cv2.resize(frame, (720, 480))\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "\n",
    "            left_eye_idx = [33, 160, 158, 133, 153, 144]\n",
    "            right_eye_idx = [263, 387, 385, 362, 380, 373]\n",
    "            mouth_idx = [13, 14]\n",
    "\n",
    "            leftEye = np.array([(landmarks[i].x, landmarks[i].y) for i in left_eye_idx])\n",
    "            rightEye = np.array([(landmarks[i].x, landmarks[i].y) for i in right_eye_idx])\n",
    "            mouth = np.array([(landmarks[i].x, landmarks[i].y) for i in mouth_idx])\n",
    "\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "            mar = mouth_aspect_ratio(mouth)\n",
    "\n",
    "            warning_text_1 = \"\"\n",
    "            warning_text_2 = \"\"\n",
    "\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "                if COUNTER == EYE_AR_CONSEC_FRAMES:\n",
    "                    ALARM_ON = True\n",
    "                    drowsiness_count += 1\n",
    "                    drowsiness_timestamps.append(time.strftime(\"%H:%M:%S\"))\n",
    "                    warning_text_1 = \"DROWSINESS ALERT!\"\n",
    "                elif COUNTER > EYE_AR_CONSEC_FRAMES:\n",
    "                    warning_text_1 = \"DROWSINESS ALERT!\"\n",
    "            else:\n",
    "                COUNTER = 0\n",
    "                ALARM_ON = False\n",
    "\n",
    "            if mar > MAR_THRESH:\n",
    "                MAR_COUNTER += 1\n",
    "                if MAR_COUNTER == MAR_CONSEC_FRAMES:\n",
    "                    yawn_alarm_on = True\n",
    "                    yawning_count += 1\n",
    "                    yawning_timestamps.append(time.strftime(\"%H:%M:%S\"))\n",
    "                    warning_text_2 = \"YAWNING DETECTED!\"\n",
    "                elif MAR_COUNTER > MAR_CONSEC_FRAMES:\n",
    "                    warning_text_2 = \"YAWNING DETECTED!\"\n",
    "            else:\n",
    "                MAR_COUNTER = 0\n",
    "                yawn_alarm_on = False\n",
    "\n",
    "            if warning_text_1:\n",
    "                cv2.putText(frame, warning_text_1, (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            if warning_text_2:\n",
    "                cv2.putText(frame, warning_text_2, (10, 60),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            cv2.putText(frame, f\"EAR: {ear:.2f}\", (500, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"MAR: {mar:.2f}\", (500, 60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Fatigue Detection\", frame)\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Report\n",
    "print(\"\\n========== FATIGUE REPORT ==========\")\n",
    "print(f\"Total Drowsiness Alerts: {drowsiness_count}\")\n",
    "print(f\"Total Yawning Alerts: {yawning_count}\")\n",
    "\n",
    "if drowsiness_timestamps:\n",
    "    print(\"\\nDrowsiness detected at:\")\n",
    "    for t in drowsiness_timestamps:\n",
    "        print(f\" - {t}\")\n",
    "\n",
    "if yawning_timestamps:\n",
    "    print(\"\\nYawning detected at:\")\n",
    "    for t in yawning_timestamps:\n",
    "        print(f\" - {t}\")\n",
    "print(\"====================================\")\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"Time\": drowsiness_timestamps + yawning_timestamps,\n",
    "    \"Warning\": [\"Drowsiness Alert\"] * len(drowsiness_timestamps) + [\"Yawning Alert\"] * len(yawning_timestamps)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"fatigue_report.csv\", index=False)\n",
    "\n",
    "print(\"Report saved as fatigue_report.csv\")\n",
    "\n",
    "# Cleanup\n",
    "vs.stream.release()\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d88e2-3846-4953-86bc-5af25cf4e457",
   "metadata": {},
   "source": [
    "## Mediapipe report saved into excel and visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6244a1c-5918-4d9b-99e7-1a1dc39314ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== FATIGUE REPORT ==========\n",
      "Total Drowsiness Alerts: 1\n",
      "Total Yawning Alerts: 0\n",
      "\n",
      "Drowsiness detected at:\n",
      " - 2025-05-19 11:58:35\n",
      "====================================\n",
      "Report saved and dashboard updated in fatigue_log.xlsx\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Constants\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 15\n",
    "MAR_THRESH = 0.09\n",
    "MAR_CONSEC_FRAMES = 15\n",
    "FRAME_SKIP = 2\n",
    "\n",
    "# Counters and flags\n",
    "frame_count = 0\n",
    "COUNTER = 0\n",
    "MAR_COUNTER = 0\n",
    "ALARM_ON = False\n",
    "yawn_alarm_on = False\n",
    "\n",
    "# Log counters and timestamps\n",
    "drowsiness_count = 0\n",
    "yawning_count = 0\n",
    "drowsiness_timestamps = []\n",
    "yawning_timestamps = []\n",
    "\n",
    "# EAR Calculation\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# MAR Calculation\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = dist.euclidean(mouth[0], mouth[1])\n",
    "    return A\n",
    "\n",
    "# Start video stream\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    frame_count += 1\n",
    "\n",
    "    if frame_count % FRAME_SKIP != 0:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame = cv2.resize(frame, (720, 480))\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "\n",
    "            left_eye_idx = [33, 160, 158, 133, 153, 144]\n",
    "            right_eye_idx = [263, 387, 385, 362, 380, 373]\n",
    "            mouth_idx = [13, 14]\n",
    "\n",
    "            leftEye = np.array([(landmarks[i].x, landmarks[i].y) for i in left_eye_idx])\n",
    "            rightEye = np.array([(landmarks[i].x, landmarks[i].y) for i in right_eye_idx])\n",
    "            mouth = np.array([(landmarks[i].x, landmarks[i].y) for i in mouth_idx])\n",
    "\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "            mar = mouth_aspect_ratio(mouth)\n",
    "\n",
    "            warning_text_1 = \"\"\n",
    "            warning_text_2 = \"\"\n",
    "\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "                if COUNTER == EYE_AR_CONSEC_FRAMES:\n",
    "                    ALARM_ON = True\n",
    "                    drowsiness_count += 1\n",
    "                    drowsiness_timestamps.append(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                    warning_text_1 = \"DROWSINESS ALERT!\"\n",
    "                elif COUNTER > EYE_AR_CONSEC_FRAMES:\n",
    "                    warning_text_1 = \"DROWSINESS ALERT!\"\n",
    "            else:\n",
    "                COUNTER = 0\n",
    "                ALARM_ON = False\n",
    "\n",
    "            if mar > MAR_THRESH:\n",
    "                MAR_COUNTER += 1\n",
    "                if MAR_COUNTER == MAR_CONSEC_FRAMES:\n",
    "                    yawn_alarm_on = True\n",
    "                    yawning_count += 1\n",
    "                    yawning_timestamps.append(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                    warning_text_2 = \"YAWNING DETECTED!\"\n",
    "                elif MAR_COUNTER > MAR_CONSEC_FRAMES:\n",
    "                    warning_text_2 = \"YAWNING DETECTED!\"\n",
    "            else:\n",
    "                MAR_COUNTER = 0\n",
    "                yawn_alarm_on = False\n",
    "\n",
    "            if warning_text_1:\n",
    "                cv2.putText(frame, warning_text_1, (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            if warning_text_2:\n",
    "                cv2.putText(frame, warning_text_2, (10, 60),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            cv2.putText(frame, f\"EAR: {ear:.2f}\", (500, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"MAR: {mar:.2f}\", (500, 60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Fatigue Detection\", frame)\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Report\n",
    "print(\"\\n========== FATIGUE REPORT ==========\")\n",
    "print(f\"Total Drowsiness Alerts: {drowsiness_count}\")\n",
    "print(f\"Total Yawning Alerts: {yawning_count}\")\n",
    "\n",
    "if drowsiness_timestamps:\n",
    "    print(\"\\nDrowsiness detected at:\")\n",
    "    for t in drowsiness_timestamps:\n",
    "        print(f\" - {t}\")\n",
    "\n",
    "if yawning_timestamps:\n",
    "    print(\"\\nYawning detected at:\")\n",
    "    for t in yawning_timestamps:\n",
    "        print(f\" - {t}\")\n",
    "print(\"====================================\")\n",
    "\n",
    "# Save log to Excel and add a visual dashboard\n",
    "data = {\n",
    "    \"timestamp\": drowsiness_timestamps + yawning_timestamps,\n",
    "    \"event\": [\"Drowsiness Alert\"] * len(drowsiness_timestamps) + [\"Yawning Alert\"] * len(yawning_timestamps)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"fatigue_log.xlsx\", index=False)\n",
    "\n",
    "# Create and save bar chart\n",
    "# Load the data\n",
    "df = pd.read_excel('fatigue_log.xlsx')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Count occurrences\n",
    "counts = df['event'].value_counts()\n",
    "\n",
    "# Create and save plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=counts.index, y=counts.values, hue=counts.index, palette=\"pastel\", legend=False)\n",
    "plt.title('Fatigue Event Counts')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Event Type')\n",
    "plt.tight_layout()\n",
    "plot_path = 'fatigue_plot.png'\n",
    "plt.savefig(plot_path)  # Save the plot as an image file\n",
    "plt.close()\n",
    "\n",
    "# Create Excel file with image embedded\n",
    "with pd.ExcelWriter('fatigue_dashboard.xlsx', engine='xlsxwriter') as writer:\n",
    "    df.to_excel(writer, sheet_name='Data', index=False)\n",
    "\n",
    "    workbook = writer.book\n",
    "    worksheet = workbook.add_worksheet('Dashboard')\n",
    "    worksheet.insert_image('B2', plot_path)  # Insert plot at cell B2\n",
    "\n",
    "print(\"Report saved and dashboard updated in fatigue_log.xlsx\")\n",
    "\n",
    "# Cleanup\n",
    "vs.stream.release()\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcd479-84f7-478a-a517-5b3a740bb59c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## IDing function added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21945f09-9005-46aa-b625-edf4fb292e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yeou1\\anaconda3\\envs\\mediapipe-env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'timestamp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 196\u001b[0m\n\u001b[0;32m    193\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Create unique filenames using timestamp\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m log_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfatigue_log_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m dashboard_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfatigue_dashboard_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Save the log file\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timestamp' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import time\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Constants\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 15\n",
    "MAR_THRESH = 0.09\n",
    "MAR_CONSEC_FRAMES = 15\n",
    "FRAME_SKIP = 2\n",
    "DEEPFACE_CHECK_INTERVAL = 300  # every 300 frames (~10 seconds at 30fps)\n",
    "\n",
    "# Counters and flags\n",
    "frame_count = 0\n",
    "COUNTER = 0\n",
    "MAR_COUNTER = 0\n",
    "ALARM_ON = False\n",
    "yawn_alarm_on = False\n",
    "\n",
    "# Log counters and timestamps\n",
    "drowsiness_log = []\n",
    "yawning_log = []\n",
    "current_name = \"Unknown\"\n",
    "\n",
    "# EAR Calculation\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# MAR Calculation\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = dist.euclidean(mouth[0], mouth[1])\n",
    "    return A\n",
    "\n",
    "# Start video stream\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    frame_count += 1\n",
    "    if frame_count % FRAME_SKIP != 0:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame = cv2.resize(frame, (720, 480))\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Face recognition using DeepFace every 300 frames\n",
    "    if frame_count % DEEPFACE_CHECK_INTERVAL == 0:\n",
    "        try:\n",
    "            bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            result = DeepFace.find(\n",
    "                img_path=bgr_frame,\n",
    "                db_path=\"known_faces\",\n",
    "                model_name=\"Facenet\",\n",
    "                distance_metric=\"cosine\",\n",
    "                enforce_detection=False,\n",
    "                silent=True,\n",
    "                threshold=0.6\n",
    "            )\n",
    "\n",
    "            if len(result) > 0 and len(result[0]) > 0:\n",
    "                identity = os.path.basename(result[0].iloc[0]['identity'])\n",
    "                current_name = os.path.splitext(identity)[0]\n",
    "                # Match found (silent)\n",
    "            else:\n",
    "                current_name = \"Unknown\"\n",
    "                # No match found (silent)\n",
    "\n",
    "        except Exception:\n",
    "            current_name = \"Unknown\"\n",
    "            # Face recognition failed (silent)\n",
    "\n",
    "    # MediaPipe facial landmarks\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "\n",
    "            left_eye_idx = [33, 160, 158, 133, 153, 144]\n",
    "            right_eye_idx = [263, 387, 385, 362, 380, 373]\n",
    "            mouth_idx = [13, 14]\n",
    "\n",
    "            leftEye = np.array([(landmarks[i].x, landmarks[i].y) for i in left_eye_idx])\n",
    "            rightEye = np.array([(landmarks[i].x, landmarks[i].y) for i in right_eye_idx])\n",
    "            mouth = np.array([(landmarks[i].x, landmarks[i].y) for i in mouth_idx])\n",
    "\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "            mar = mouth_aspect_ratio(mouth)\n",
    "\n",
    "            warning_text_1 = \"\"\n",
    "            warning_text_2 = \"\"\n",
    "\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "                if COUNTER == EYE_AR_CONSEC_FRAMES:\n",
    "                    ALARM_ON = True\n",
    "                    drowsiness_log.append((time.strftime(\"%Y-%m-%d %H:%M:%S\"), current_name))\n",
    "                    warning_text_1 = \"DROWSINESS ALERT!\"\n",
    "                elif COUNTER > EYE_AR_CONSEC_FRAMES:\n",
    "                    warning_text_1 = \"DROWSINESS ALERT!\"\n",
    "            else:\n",
    "                COUNTER = 0\n",
    "                ALARM_ON = False\n",
    "\n",
    "            if mar > MAR_THRESH:\n",
    "                MAR_COUNTER += 1\n",
    "                if MAR_COUNTER == MAR_CONSEC_FRAMES:\n",
    "                    yawn_alarm_on = True\n",
    "                    yawning_log.append((time.strftime(\"%Y-%m-%d %H:%M:%S\"), current_name))\n",
    "                    warning_text_2 = \"YAWNING DETECTED!\"\n",
    "                elif MAR_COUNTER > MAR_CONSEC_FRAMES:\n",
    "                    warning_text_2 = \"YAWNING DETECTED!\"\n",
    "            else:\n",
    "                MAR_COUNTER = 0\n",
    "                yawn_alarm_on = False\n",
    "\n",
    "            if warning_text_1:\n",
    "                cv2.putText(frame, warning_text_1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            if warning_text_2:\n",
    "                cv2.putText(frame, warning_text_2, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            cv2.putText(frame, f\"EAR: {ear:.2f}\", (600, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"MAR: {mar:.2f}\", (600, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Show current recognized name with color based on match status\n",
    "    name_color = (0, 255, 0) if current_name != \"Unknown\" else (0, 0, 255)\n",
    "    cv2.putText(frame, current_name, (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, name_color, 2)\n",
    "\n",
    "    cv2.imshow(\"Fatigue Detection\", frame)\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Create and save report\n",
    "data = {\n",
    "    \"timestamp\": [ts for ts, _ in drowsiness_log + yawning_log],\n",
    "    \"person\": [name for _, name in drowsiness_log + yawning_log],\n",
    "    \"event\": [\"Drowsiness Alert\"] * len(drowsiness_log) + [\"Yawning Alert\"] * len(yawning_log)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"fatigue_log.xlsx\", index=False)\n",
    "\n",
    "# Create and save plot\n",
    "counts = df['event'].value_counts()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=counts.index, y=counts.values, hue=counts.index, palette=\"pastel\", legend=False)\n",
    "plt.title('Fatigue Event Counts')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Event Type')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fatigue_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plotting\n",
    "# Convert timestamp column to datetime if not already\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Round timestamps to the nearest minute for grouping (or use .dt.floor('10s') for 10-sec bins)\n",
    "df['minute'] = df['timestamp'].dt.floor('T')\n",
    "\n",
    "# Group by time and event\n",
    "time_counts = df.groupby(['minute', 'event']).size().reset_index(name='count')\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=time_counts, x='minute', y='count', hue='event', palette=\"Set2\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Fatigue Events Over Time')\n",
    "plt.ylabel('Event Count')\n",
    "plt.xlabel('Time')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fatigue_time_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# Create unique filenames using timestamp\n",
    "log_filename = f\"fatigue_log_{timestamp}.xlsx\"\n",
    "dashboard_filename = f\"fatigue_dashboard_{timestamp}.xlsx\"\n",
    "\n",
    "# Save the log file\n",
    "df.to_excel(log_filename, index=False)\n",
    "\n",
    "# Save Excel dashboard with embedded plot\n",
    "with pd.ExcelWriter(dashboard_filename, engine='xlsxwriter') as writer:\n",
    "    df.to_excel(writer, sheet_name='Data', index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = workbook.add_worksheet('Dashboard')\n",
    "    worksheet.insert_image('B2', 'fatigue_plot.png')\n",
    "    worksheet.insert_image('B22', 'fatigue_time_plot.png')  # adjust position if needed\n",
    "    \n",
    "print(f\"Files saved as {log_filename} and {dashboard_filename}\")\n",
    "\n",
    "vs.stream.release()\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f671c64-636e-4856-b047-f2389bb3255c",
   "metadata": {},
   "source": [
    "## Recorded video timestamping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8cb36742-bd82-4903-9354-5893a8ca6c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard saved as fatigue_dashboard.xlsx\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import time\n",
    "from scipy.spatial import distance as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Constants\n",
    "EYE_AR_THRESH = 0.37\n",
    "EYE_AR_CONSEC_FRAMES = 10\n",
    "MAR_THRESH = 0.07\n",
    "MAR_CONSEC_FRAMES = 10\n",
    "FRAME_SKIP = 1\n",
    "DEEPFACE_CHECK_INTERVAL = 300  # ~10 seconds at 30fps\n",
    "\n",
    "# Counters and flags\n",
    "frame_count = 0\n",
    "COUNTER = 0\n",
    "MAR_COUNTER = 0\n",
    "ALARM_ON = False\n",
    "yawn_alarm_on = False\n",
    "\n",
    "# Log counters and timestamps\n",
    "drowsiness_log = []\n",
    "yawning_log = []\n",
    "current_name = \"Unknown\"\n",
    "\n",
    "# EAR Calculation\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "# MAR Calculation\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    return dist.euclidean(mouth[0], mouth[1])\n",
    "\n",
    "# Load video file\n",
    "video_path = \"C:/Users/yeou1/Fatigue_project/Accuracy/test_2.mp4\"  # Update with your file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file {video_path}\")\n",
    "    exit()\n",
    "\n",
    "# Get video FPS and compute frame delay\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_delay = int(1000 / fps) if fps > 0 else 33  # default to ~30fps\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % FRAME_SKIP != 0:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.resize(frame, (720, 480))\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # DeepFace ID every N frames\n",
    "    if frame_count % DEEPFACE_CHECK_INTERVAL == 0:\n",
    "        try:\n",
    "            bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            result = DeepFace.find(\n",
    "                img_path=bgr_frame,\n",
    "                db_path=\"known_faces\",\n",
    "                model_name=\"Facenet\",\n",
    "                distance_metric=\"cosine\",\n",
    "                enforce_detection=False,\n",
    "                silent=True,\n",
    "                threshold=0.6\n",
    "            )\n",
    "            if len(result) > 0 and len(result[0]) > 0:\n",
    "                identity = os.path.basename(result[0].iloc[0]['identity'])\n",
    "                current_name = os.path.splitext(identity)[0]\n",
    "            else:\n",
    "                current_name = \"Unknown\"\n",
    "        except Exception:\n",
    "            current_name = \"Unknown\"\n",
    "\n",
    "    # MediaPipe facial landmarks\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "\n",
    "            left_eye_idx = [33, 160, 158, 133, 153, 144]\n",
    "            right_eye_idx = [263, 387, 385, 362, 380, 373]\n",
    "            mouth_idx = [13, 14]\n",
    "\n",
    "            leftEye = np.array([(landmarks[i].x, landmarks[i].y) for i in left_eye_idx])\n",
    "            rightEye = np.array([(landmarks[i].x, landmarks[i].y) for i in right_eye_idx])\n",
    "            mouth = np.array([(landmarks[i].x, landmarks[i].y) for i in mouth_idx])\n",
    "\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "            mar = mouth_aspect_ratio(mouth)\n",
    "\n",
    "            # Get video timestamp\n",
    "            current_time_sec = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "            timestamp = time.strftime(\"%H:%M:%S\", time.gmtime(current_time_sec))\n",
    "\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "                if COUNTER == EYE_AR_CONSEC_FRAMES:\n",
    "                    ALARM_ON = True\n",
    "                    drowsiness_log.append((timestamp, current_name))\n",
    "            else:\n",
    "                COUNTER = 0\n",
    "                ALARM_ON = False\n",
    "\n",
    "            if mar > MAR_THRESH:\n",
    "                MAR_COUNTER += 1\n",
    "                if MAR_COUNTER == MAR_CONSEC_FRAMES:\n",
    "                    yawn_alarm_on = True\n",
    "                    yawning_log.append((timestamp, current_name))\n",
    "            else:\n",
    "                MAR_COUNTER = 0\n",
    "                yawn_alarm_on = False\n",
    "\n",
    "            # Overlay data on frame (optional for debugging)\n",
    "            cv2.putText(frame, f\"EAR: {ear:.2f}\", (600, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"MAR: {mar:.2f}\", (600, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    name_color = (0, 255, 0) if current_name != \"Unknown\" else (0, 0, 255)\n",
    "    cv2.putText(frame, current_name, (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, name_color, 2)\n",
    "\n",
    "    # Show video during processing\n",
    "    cv2.imshow(\"Fatigue Detection (Video)\", frame)\n",
    "    if cv2.waitKey(frame_delay) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Save fatigue event logs\n",
    "data = {\n",
    "    \"timestamp\": [ts for ts, _ in drowsiness_log + yawning_log],\n",
    "    \"person\": [name for _, name in drowsiness_log + yawning_log],\n",
    "    \"event\": [\"Drowsiness Alert\"] * len(drowsiness_log) + [\"Yawning Alert\"] * len(yawning_log)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"fatigue_log.xlsx\", index=False)\n",
    "\n",
    "# Plot event counts\n",
    "counts = df['event'].value_counts()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=counts.index, y=counts.values, hue=counts.index, palette=\"pastel\", legend=False)\n",
    "plt.title('Fatigue Event Counts')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Event Type')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fatigue_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save Excel dashboard\n",
    "with pd.ExcelWriter('fatigue_dashboard.xlsx', engine='xlsxwriter') as writer:\n",
    "    df.to_excel(writer, sheet_name='Data', index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = workbook.add_worksheet('Dashboard')\n",
    "    worksheet.insert_image('B2', 'fatigue_plot.png')\n",
    "\n",
    "print(\"Dashboard saved as fatigue_dashboard.xlsx\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc26609-8048-4158-8afb-4a11fbc0606e",
   "metadata": {},
   "source": [
    "## Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69287580-715c-4d53-b4a5-280f80274f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 8  2]\n",
      " [12  0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     fatigue       0.40      0.80      0.53        10\n",
      "  no_fatigue       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.36        22\n",
      "   macro avg       0.20      0.40      0.27        22\n",
      "weighted avg       0.18      0.36      0.24        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load files\n",
    "ground_df = pd.read_excel(\"ground_truth.xlsx\") #the file is in Accuracy folder\n",
    "pred_df = pd.read_excel(\"fatigue_log.xlsx\") #load the excel file produced from the timestamping\n",
    "\n",
    "def timestamp_to_sec(t):\n",
    "    # If it's already a datetime.time object\n",
    "    if hasattr(t, \"hour\"):\n",
    "        return t.hour * 3600 + t.minute * 60 + t.second\n",
    "    # If it's a string like \"HH:MM:SS\"\n",
    "    try:\n",
    "        parsed_time = datetime.strptime(t, \"%H:%M:%S\").time()\n",
    "        return parsed_time.hour * 3600 + parsed_time.minute * 60 + parsed_time.second\n",
    "    except:\n",
    "        return 0  # or raise error/log if unexpected format\n",
    "\n",
    "ground_df['seconds'] = ground_df['timestamp'].apply(timestamp_to_sec)\n",
    "pred_df['seconds'] = pred_df['timestamp'].apply(timestamp_to_sec)\n",
    "# Match flags\n",
    "matched_preds = set()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Tolerance window\n",
    "TOLERANCE = 2\n",
    "\n",
    "# Match ground truth to predictions\n",
    "for idx, row in ground_df.iterrows():\n",
    "    gt_time = row['seconds']\n",
    "    gt_name = row['person']\n",
    "    gt_event = row['event']\n",
    "\n",
    "    # Check for a matching prediction within ±2 sec\n",
    "    match = pred_df[\n",
    "        (abs(pred_df['seconds'] - gt_time) <= TOLERANCE) &\n",
    "        (pred_df['person'] == gt_name) &\n",
    "        (pred_df['event'] == gt_event)\n",
    "    ]\n",
    "\n",
    "    if not match.empty:\n",
    "        matched_idx = match.index[0]\n",
    "        matched_preds.add(matched_idx)\n",
    "        y_true.append('fatigue')\n",
    "        y_pred.append('fatigue')  # True Positive\n",
    "    else:\n",
    "        y_true.append('fatigue')\n",
    "        y_pred.append('no_fatigue')  # False Negative\n",
    "\n",
    "# Check for unmatched predictions (False Positives)\n",
    "for idx, row in pred_df.iterrows():\n",
    "    if idx not in matched_preds:\n",
    "        y_true.append('no_fatigue')\n",
    "        y_pred.append('fatigue')  # False Positive\n",
    "\n",
    "# Compute and display metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=['fatigue', 'no_fatigue']))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=['fatigue', 'no_fatigue']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a35ea-eb11-4e7e-aa7d-211e5cace607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
